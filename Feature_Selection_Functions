import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from handy import df_preprossecing
# from scipy.stats import kruskal,f_oneway
from scipy.stats.mstats import kruskalwallis
from scipy.optimize import dual_annealing
from pingouin import kruskal
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, RFE, RFECV, f_classif, \
    mutual_info_classif, SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.inspection import permutation_importance
from sklearn.pipeline import Pipeline
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.decomposition import PCA
from sklearn import svm
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from itertools import combinations
from xgboost import XGBClassifier
from boruta import BorutaPy
# from genetic_selection import GeneticSelectionCV
from gafs import SklearnGeneticSelection

# Koden starter linje ca 600
def getFeatureList(n_splits, glob=False, filtered=False):
    if filtered == False:
        loc = 'Features_All'
    else:
        loc = 'Features_All_w_Filtering'
    if glob==False:
        df = pd.read_pickle('Features/%s/n_splits_%i_sensor_AL02.pkl'%(loc,n_splits))
    else:
        df = pd.read_pickle('Features/%s/n_splits_%i_global.pkl'%(loc,n_splits))
    features = df.columns
    features = features.drop('Damage')
    return features
def getSensorList(sensor_group='all'):
    Local = ['AL01', 'AL02', 'AL03', 'AL04', 'AL05', 'AL06', 'AL07',
             'AL08', 'AL09', 'AL10', 'AL11', 'AL12', 'AL13', 'AL14', 'AL15', 'AL16',
             'AL17', 'AL18', 'AL19', 'AL20', 'AL21', 'AL22', 'AL23', 'AL24', 'AL25',
             'AL26', 'AL27', 'AL28', 'AL29', 'AL30', 'AL31', 'AL32', 'AL33', 'AL34',
             'AL35', 'AL36', 'AL37', 'AL38', 'AL39', 'AL40']
    Globalx = ['AG01x',
               'AG02x', 'AG03x', 'AG04x', 'AG05x', 'AG06x', 'AG07x', 'AG08x', 'AG09x',
               'AG10x', 'AG11x', 'AG12x', 'AG13x', 'AG14x', 'AG15x', 'AG16x', 'AG17x', 'AG18x']
    Globalz = ['AG01z',
               'AG02z', 'AG03z', 'AG04z', 'AG05z', 'AG06z', 'AG07z', 'AG08z',
               'AG10z', 'AG11z', 'AG12z', 'AG13z', 'AG14z', 'AG15z', 'AG16z', 'AG17z', 'AG18z']
    if sensor_group.lower() == 'all':
        list = Local + Globalz + Globalx
    elif sensor_group[0].lower() == 'g' and sensor_group[-1].lower() == 'x':
        list = Globalx
    elif sensor_group[0].lower() == 'g' and sensor_group[-1].lower() == 'z':
        list = Globalz
    elif sensor_group[0].lower() == 'g':
        list = Globalx + Globalz
    elif sensor_group[0].lower() == 'l':
        list = Local
    return list

def chooseFeatures(df, wantedFeatures):
    # Excludes features that are not requested
    # Output: updated dataframe
    if (type(wantedFeatures) == pd.core.indexes.base.Index):
        wantedFeatures = wantedFeatures.tolist()
    allFeatures = df.columns
    wantedFeatures.append('Damage')
    for i in allFeatures:
        if i not in wantedFeatures:
            df = df.drop([i], 1)
    return df
def combineSensors(sensors, n_splits, wantedFeatures):
    # if (len(sensors) <= 1):
    #     df = pd.read_pickle('Features/Features_All/n_splits_50_sensor_AL02.pkl')
    #     df = chooseFeatures(df,wantedFeatures)
    #     return df
    df = pd.DataFrame()
    damage = pd.read_pickle('Features/Features_All/n_splits_' + str(n_splits) + '_sensor_' + sensors[0] + '.pkl')[
        'Damage']
    for sensorN in sensors:
        temp_df = pd.read_pickle('Features/Features_All/n_splits_' + str(n_splits) + '_sensor_' + sensorN + '.pkl')
        temp_df = chooseFeatures(temp_df, wantedFeatures)
        temp_df = temp_df.drop('Damage', axis=1)
        temp_df = temp_df.add_suffix('_' + sensorN)
        df = pd.concat([df, temp_df], axis=1)
    df = pd.concat([df, damage], axis=1)
    return df

def add_interactions(df):
    # Get feature names
    combos = list(combinations(list(df.columns), 2))
    colnames = list(df.columns) + ['_'.join(x) for x in combos]
    # Find interactions. Kan vurdere Ã¥ bruke en annen kombinasjon enn polynomial
    poly = PolynomialFeatures(interaction_only=True, include_bias=False)
    df = poly.fit_transform(df)
    df = pd.DataFrame(df)
    df.columns = colnames
    # Remove interaction terms with all 0 values
    noint_indicies = [i for i, x in enumerate(list((df == 0).all())) if x]
    df = df.drop(df.columns[noint_indicies], axis=1)
    return df

def createFigure(rankings, threshold, ylabel='', title='', vertical=True):
    plt.figure(figsize=(9,5))
    if vertical==True:
        plt.plot(rankings.index, threshold * np.ones(rankings.size), linestyle='--', color='red', label='threshold')
        plt.legend()
        rankings.plot.bar()
        plt.ylabel(ylabel)
        plt.yticks(rotation=45, va='top', ha='right', rotation_mode='anchor')
    else:
        plt.plot(threshold * np.ones(rankings.size), rankings.index, linestyle='--', color='red', label='threshold')
        plt.legend()
        rankings.plot.barh()
        plt.gca().invert_yaxis()
        plt.xlabel(ylabel)
    plt.title(title)
    plt.tight_layout()
    plt.show()
def thresholdEliminate(rankings, threshold, greaterThan=True):
    if greaterThan == False:
        eliminate = rankings[rankings >= threshold]
        keep = rankings[rankings < threshold]
    else:
        eliminate = rankings[rankings <= threshold]
        keep = rankings[rankings > threshold]
    return keep, eliminate
def createFigure_k(keep, eliminate, score, title='', ylabel='', vertical=True):
    plt.figure(figsize=(8,5))
    if vertical==True:
        plt.bar(keep, score[keep], width=0.6)
        plt.bar(eliminate, score[eliminate], color='red', width=0.6)
        plt.ylabel(ylabel)
        plt.xticks(rotation=45, va='top', ha='right', rotation_mode='anchor')
    else:
        plt.barh(keep, score[keep])
        plt.barh(eliminate, score[eliminate], color='red')
        plt.gca().invert_yaxis()
        plt.xlabel(ylabel)
    plt.title(title)
    plt.tight_layout()
    plt.show()

def ML_method(method):  # LogReg, DecisionTree, RandomForest, ExtremeTree, XGB
    if method == 'LogReg':
        clf = LogisticRegression(max_iter=150, tol=1e-4, C=5, verbose=0)
    elif method == 'DecisionTree':
        clf = DecisionTreeClassifier()
    elif method == 'RandomForest':
        clf = RandomForestClassifier(bootstrap=True, criterion='gini', max_depth=4, min_samples_leaf=2, min_samples_split=5,  n_estimators=100, n_jobs=1)  # , max_features=1
    elif method == 'ExtremeTree':
        clf = ExtraTreesClassifier()
    elif method == 'XGB':
        clf = XGBClassifier(min_child_weight=1, subsample=0.65, max_depth=5, learning_rate=0.4, gamma=0.4, reg_lambda=3, colsample_bytree=0.3, n_estimators=50,  use_label_encoder=False, n_jobs=1, cv=5, verbosity=0)
    return clf

# ----------------Filter Methods-----------------
def check_variance_features(df, threshold=0.0, showElim=False):
    X = df.drop(['Damage'], 1)  # Verify if the features contains the damage
    y = df.Damage
    var_tresh = VarianceThreshold(threshold=threshold)
    var_tresh.fit(X)
    var_tresh.get_support()
    eliminate = X.columns[var_tresh.get_support()==False]
    keep = X.columns[var_tresh.get_support()]
    if showElim == True:
        print('Eliminated by Constant: %i'%len(eliminate))
        print(eliminate)
    X = X[keep]
    X['Damage'] = y
    return X

def anova_FS(df, p_value=0.05, showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    sel = f_classif(X, y)  # Returns the f-score and the p-value
    p_values = pd.Series(sel[1], index=X.columns)
    p_values.sort_values(ascending=True, inplace=True)
    keep, eliminate = thresholdEliminate(p_values, p_value, greaterThan=False)
    if showFig == True:
        createFigure(p_values, p_value, ylabel='p-value', title='ANOVA, p-value',vertical=False)
    if showElim == 1:
        print('Eliminated by ANOVA: %i'%len(eliminate.index))
        print(eliminate.index)
    X = X[keep.index]
    X['Damage'] = y
    return X

def kruskal_wallis(df, p_value=0.05, showFig=False, showElim=False):
    # WORK IN PROGRESS
    X = df.drop(['Damage'], 1)
    y = df.Damage
    data = []
    for i in range(X.shape[1]):
        tempFeat = X.columns[i]
        sel = kruskal(X[X.columns[i]], y)
        data.append(sel[1])
    p_values = pd.Series(data, index=X.columns)
    p_values.sort_values(ascending=True, inplace=True)
    # print(p_values)
    keep, eliminate = thresholdEliminate(p_values, p_value, greaterThan=False)
    if showFig == True:
        createFigure(p_values, p_value, ylabel='p-value', title='Kruskal Wallis, p-value')
    if showElim == 1:
        print('Eliminated by Kruskal Wallis: %i'%len(eliminate.index))
        print(eliminate.index)
    X = X[keep.index]
    X['Damage'] = y
    return X

def uni_correlation(df, threshold=0.1, method='pearson', absVal=True, showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    uni_corr = X.corrwith(y,method=method)
    if absVal == True:
        uni_corr = abs(uni_corr)
    uni_corr = uni_corr.sort_values(ascending=False)
    keep, eliminate = thresholdEliminate(uni_corr, threshold, greaterThan=True)
    if showFig == True:
        createFigure(uni_corr, threshold, ylabel='Correlation', title='Correlation with Damage, ' + method.capitalize(), vertical=False)
    if showElim == 1:
        print('Eliminated by univariate correlation: %i'%len(eliminate.index))
        print(eliminate.index)
    X = X[keep.index]
    X['Damage'] = y
    return X, uni_corr

def mutual_information_gain_FS(df, threshold=0.1, showFig=False, showElim=False):
        # Finds the correlation between the input and output variables
    X = df.drop(['Damage'], 1)
    y = df.Damage
    mig = mutual_info_classif(X, y)
    mig = pd.Series(mig, index=X.columns)
    mig.sort_values(ascending=False, inplace=True)
    keep, eliminate = thresholdEliminate(mig, threshold, greaterThan=True)
    if showFig == True:
        createFigure(mig, threshold, ylabel='Mutual Information, I', title='Mutual Information Gain', vertical=True)
    if showElim == 1:
        print('Eliminated by Mutual Information Gain: %i'%len(eliminate.index))
        print(eliminate.index)
    X = X[keep.index]
    X['Damage'] = y
    return X, mig

def ROC_AUC(df, threshold=0.1, ML='RandomForest', testSize=0.5, avgMethod='macro', multiClass='ovo', auc_mod=True,
            showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testSize, stratify=y)  # , random_state=10
    auc = []
    for feature in X_train.columns:
        clf = ML_method(ML)
        clf.fit(X_train[feature].to_frame(), y_train)
        y_pred_proba = clf.predict_proba(X_test[feature].to_frame())
        auc.append(roc_auc_score(y_test, y_pred_proba, multi_class=multiClass, average=avgMethod))
    auc_values = pd.Series(auc, index=X.columns)
    ylabel = 'AUC'
    if auc_mod == True:
        auc_values = abs(0.5 - auc_values)
        ylabel = '|0.5 - AUC|'
    auc_values.sort_values(ascending=False, inplace=True)
    keep, eliminate = thresholdEliminate(auc_values, threshold, greaterThan=True)
    if showFig == True:
        createFigure(auc_values, threshold, ylabel=ylabel, title='AUC-ranking using ' + ML)
    if showElim == True:
        print('Eliminated by AUC-ROC: %i'%len(eliminate.index))
        print(eliminate.index)
    X = X[keep.index]
    X['Damage'] = y
    return X

def model_based_ranking(df, threshold=0.3, ML='RandomForest', testSize=0.5, showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testSize, stratify=y)  # , random_state=10
    ranking = []
    for feature in X_train.columns:
        clf = ML_method(ML)
        clf.fit(X_train[feature].to_frame(), y_train)
        y_pred = clf.predict(X_test[feature].to_frame())
        ranking.append(accuracy_score(y_test, y_pred))
    ranking = pd.Series(ranking, index=X.columns)
    ranking.sort_values(ascending=False, inplace=True)
    keep, eliminate = thresholdEliminate(ranking, threshold, greaterThan=True)
    if showFig == True:
        createFigure(ranking, threshold, ylabel='Predictive accuracy',
                     title='Predicive accuracy for each feature using ' + ML)
    if showElim == True:
        print('Eliminated by Model Based Ranking: %i'%len(eliminate.index))
        print(eliminate.index)
    X = X[keep.index]
    X['Damage'] = y
    return X, ranking

def find_correlated_features(corrmat, threshold):
    corrmat=abs(corrmat)
    col_corr = set()
    for i in range(len(corrmat.columns)):
        for j in range(i):
            if (corrmat.iloc[i, j]) > threshold and i!=j:
                colname1 = corrmat.columns[i]
                colname2 = corrmat.columns[j]
                col_corr.add(colname1)
                col_corr.add(colname2)
    return col_corr
def corr_grouping(corrmat, threshold, showGroups=False):
    corrdata = corrmat.abs().stack()
    corrdata = corrdata.sort_values(ascending=False)
    corrdata = corrdata[corrdata>threshold]
    corrdata = corrdata[corrdata<1]
    corrdata = pd.DataFrame(corrdata).reset_index()
    corrdata.columns = ['feature1','feature2','corr_value']
    grouped_feature_list = []
    correlated_groups_list = []
    for feature in corrdata.feature1.unique():
        if feature not in grouped_feature_list:
            correlated_block = corrdata[corrdata.feature1 == feature]
            grouped_feature_list = grouped_feature_list + list(correlated_block.feature2.unique())
            correlated_groups_list.append(correlated_block)
    if showGroups==True:
        for group in correlated_groups_list:
            print('Groups:')
            print(group)
        print('Total Groups: %i'%(len(correlated_groups_list)))
    return correlated_groups_list
def corr_group_feature_importance(correlated_groups_list, corr_features, X, y, showSelection=False):
    important_features = []
    for group in correlated_groups_list:
        features = list(group.feature1.unique()) + list(group.feature2.unique())
        # print(features)
        df_group = X[features]
        df_group['Damage'] = y
            # method1 = 'mig' , 'auc' , 'modelBased' , 'treeBased'
        df_group = getBestFeature(df_group,method1='mig',method2='RandomForest')
        best_feature = df_group.columns[0]
        important_features.append(best_feature)
    # print(important_features)
    features_to_discard = set(corr_features) - set(important_features)
    features_to_discard = list(features_to_discard)
    if showSelection == True:
        print('Features to Discard:', features_to_discard)
        print('Correlating Features to Keep:', important_features)
    return features_to_discard
def getBestFeature(df, method1, method2='RandomForest'):    #method1 = 'mig' , 'auc' , 'modelBased' , 'treeBased'
    showFig=False
    if method1 == 'mig':
        df_return = mutual_information_gain_FS(df,threshold=0,showFig=showFig)[0]
    elif method1 == 'auc':
        df_return = ROC_AUC(df, threshold=0, ML=method2, auc_mod=True,showFig=showFig)
    elif method1 == 'modelBased':
        df_return = model_based_ranking(df, threshold=0, ML=method2,showFig=showFig)
    elif method1=='treeBased':
        df_return = tree_based_FS(df,k=1,ML=method2,importance_method='MDI',showFig=showFig)
    return df_return

def multi_correlation_V2(df, threshold=0.85, corr_method='pearson', showGroups=False, showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    corrmat = X.corr(method=corr_method)
    corr_features = find_correlated_features(corrmat,threshold)
    correlated_groups_list=corr_grouping(corrmat,threshold,showGroups=showGroups)
    corr_features_to_discard = corr_group_feature_importance(correlated_groups_list,corr_features,X,y,showSelection=False)
    if showFig == True:
        plt.figure()
        sns.heatmap(corrmat, annot=True, cmap=plt.cm.hot_r)
        plt.title('Correlating Features, '+corr_method.capitalize())
        plt.show()
    if showElim == True:
        print('Eliminated by Correlation Method: %i' %len(corr_features_to_discard))
        print(corr_features_to_discard)
    X = X.drop(corr_features_to_discard, axis=1)
    X['Damage'] = y
    return X, corr_features_to_discard

def multi_correlation_V3(df, threshold=0.85, corr_method='pearson', showGroups=False, showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    corrmat = X.corr(method=corr_method)
    corr_features = find_correlated_features(corrmat,threshold)
    correlated_groups_list=corr_grouping(corrmat,threshold,showGroups=showGroups)
    corr_features_to_discard = corr_group_feature_importance(correlated_groups_list,corr_features,X,y,showSelection=False)
    if showFig == True:
        plt.figure()
        sns.heatmap(corrmat, annot=True, cmap=plt.cm.hot_r)
        plt.title('Correlating Features, '+corr_method.capitalize())
        plt.show()
    if showElim == True:
        print('Eliminated by Correlation Method: %i' %len(corr_features_to_discard))
        print(corr_features_to_discard)
    X = X.drop(corr_features_to_discard, axis=1)
    X['Damage'] = y
    return X, corr_features_to_discard, corrmat

# ----------------Wrapper Methods-----------------
def wrapper_FS(df, forward=True, method='DecisionTree', keep_k=0, floating=False, showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    clf = ML_method(method)
    if keep_k == 0 or keep_k>len(X.columns):
        keep_k = len(X.columns)
    wrapper = SFS(clf, k_features=(1, keep_k), forward=forward, floating=floating, scoring='accuracy', cv=4, n_jobs=1)
    wrapper = wrapper.fit(X, y)
    df_info = pd.DataFrame.from_dict(wrapper.get_metric_dict()).T
    feature_sets = list(df_info.feature_names)
    score = list(df_info.avg_score)
    best_score = wrapper.k_score_
    best_feature_set = wrapper.k_feature_names_
    feature_order = []
    if forward:
        i_lim = 0
        step = 'Forward'
    else:
        i_lim = keep_k - 1
        step = 'Backward'
    for i in range(keep_k):
        if i == i_lim:
            feature_order.append(''.join(set(feature_sets[i])))
            continue
        if forward == True:
            j = i - 1
        else:
            j = i + 1
        feature_order.append(''.join(set(feature_sets[i]).difference(feature_sets[j])))
    num_features = score.index(best_score)+1
    lastFeature = feature_order[num_features-1]
    score = pd.Series(score,index=feature_order)
    keep = score[0:num_features].index
    eliminate = score[num_features:].index
    if forward == False:
        keep, eliminate = eliminate, keep
    if keep_k != len(X.columns):
        ignored_features = list(set(X.columns).symmetric_difference(set(feature_order)))
        print('Ignored Features (Wrapper):')
        print(ignored_features)
    if showFig == True:
        title = step + ' Feature Selection, \u279E'
        createFigure_k(keep,eliminate,score,title,'Accuracy',vertical=False)
        # plt.barh(lastFeature, best_score)
        plt.xlim([0, 1])
        fig1 = plot_sfs(wrapper.get_metric_dict(), kind='std_dev')
        plt.grid()
        plt.title(step + ' Feature Selection, \u279E, (w.std_dev)')
        plt.ylabel('Accuracy')
    if showElim == True:
        print('Features Eliminated from %s Feature Selection: %i'%(step,eliminate))
        print(eliminate)
    X = X[keep]
    X['Damage'] = y
    return X

# ----------------Embedded Methods-----------------
def regularisation_FS(df, regu_method='L1',showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    if regu_method=='L1':
        clf = LogisticRegression(penalty='l1',C=0.05,solver='saga',multi_class='ovr')
    elif regu_method == 'L2':
        clf = LogisticRegression(penalty='l2',C=1,solver='saga',multi_class='ovr')    #class_weight='balanced'
    elif regu_method == 'ElasticNet':
        clf = LogisticRegression(penalty='elasticnet',C=1,solver='saga', l1_ratio=0.5)
    sel = SelectFromModel(clf)
    sel.fit(X, y)
    coef_score = (np.mean(abs(sel.estimator_.coef_),axis=0))
    coef_score = pd.Series(coef_score,index=X.columns)
    coef_score = coef_score.sort_values(ascending=False)
    num_features = sum(sel.get_support())
    selected_features = coef_score[0:num_features].index
    elim_features = coef_score[num_features:].index
    if showFig==True:
        title='Coefficient Score using %s Regularization' %(regu_method)
        createFigure_k(selected_features,elim_features,coef_score,title,'Score')
    if showElim == True:
        print('Features eliminated from %s Regularization: %i' % (regu_method,len(elim_features)))
        print(elim_features)
    X = X[selected_features]
    X['Damage'] = y
    return X, coef_score, elim_features

def kNN_importance(df, k=0, fig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    if k == 0 or k > len(X.columns):
        k = len(X.columns)
    model = KNeighborsClassifier()
    model.fit(X, y)
    results = permutation_importance(model, X, y, n_repeats=10,n_jobs=-1, scoring='accuracy') #
    importance = results.importances_mean
    # Creating a dataframe with the results
    importance = pd.Series(importance,index = X.columns)
    importance = importance.sort_values(ascending=False)
    selected_features = importance[0:k].index
    elim_features = importance[k:].index
    if fig == True:
        createFigure_k(selected_features,elim_features,importance,'kNN Importance','Importance')
    if showElim == True:
        print('Features eliminated from kNN importance, k = %i:' %k)
        print(elim_features)
    X = X[selected_features]
    X['Damage'] = y
    return X, importance

def tree_based_FS(df, k=0, ML='RandomForest', importance_method='MDA', showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    clf = ML_method(ML)
    if clf == 'XGB':
        clf = XGBClassifier(reg_alpha=1, reg_lambda=0)
    clf.fit(X, y)
    if ML == 'LogReg':
        importance = clf.coef_[0]
        importance_method = 'MDI'
    else:
        if importance_method == 'MDI':
            importance = clf.feature_importances_
        elif importance_method == 'MDA':
            importance_MDA = permutation_importance(clf, X, y, n_repeats=10)
            sorted_idx = importance_MDA.importances_mean.argsort()
            importance = importance_MDA.importances_mean
    importance = pd.Series(importance, index=X.columns)
    importance = importance.sort_values(ascending=False)
    if k == 0 or k > len(X.columns):
        k = len(X.columns)
    selected_features = importance[0:k].index
    elim_features = importance[k:].index
    if showFig == True:
        title = 'Feature Importance (' + importance_method + ') using ' + ML
        createFigure_k(selected_features,elim_features,importance,title,'Feature Importance',vertical=True)
        if importance_method == 'MDA':
            plt.figure()
            plt.boxplot(importance_MDA.importances[sorted_idx].T, vert=False, labels=X.columns[sorted_idx])
            plt.title('Permutation Importance')
    if showElim == True:
        print('Features eliminated from %s importance, k = %i:' %(ML,k) )
        print(elim_features)
    X = X[selected_features]
    X['Damage'] = y
    return X, importance, elim_features

def boruta_FS(df, ML='ExtremeTree', showFig=False, showElim=True):
    X = np.array(df.drop(['Damage'], 1))
    y = np.array(df.Damage)
    clf = ML_method(ML)  # RandomForest, ExtremeTree, XGB
    feature_selector = BorutaPy(clf, n_estimators='auto', verbose=2)
    feature_selector.fit(X, y)
    X_filtered = feature_selector.transform(X)
    feature_names = df.drop(['Damage'], 1).columns
    feature_ranks = list(zip(feature_names, feature_selector.ranking_, feature_selector.support_))
    col_names = ['Feature', 'Ranking', 'Keep']
    boruta_df = pd.DataFrame(feature_ranks, columns=col_names)
    boruta_df = boruta_df.sort_values(by=['Ranking'],ignore_index=True)
    print(boruta_df)
    ranking = pd.Series(feature_selector.ranking_, feature_names)
    ranking = ranking.sort_values(ascending=True)
    keep = []
    eliminate = []
    for index, element in enumerate(boruta_df.Keep):
        if element == True:
            keep.append(boruta_df.Feature[index])
        else:
            eliminate.append(boruta_df.Feature[index])
    if showFig == True:
        title = 'Boruta Feature Importance, ' + ML
        createFigure_k(keep,eliminate,ranking,title,'Ranking',vertical=False)
    if showElim == True:
        print('Eliminated by Boruta: %i'%len(eliminate))
        print(eliminate)
    X = pd.DataFrame(X_filtered, columns=keep)
    X['Damage'] = df.Damage
    return X, ranking, eliminate, feature_selector

# ----------------Hybrid Methods-----------------
def recursive_feature_elimination(df, cv=True, rankLim = 0, method='RandomForest', showFig=False, showElim=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    clf=ML_method(method)
    if cv == False:
        recursive = RFE(clf, n_features_to_select=1)
        titleText = ''
    else:
        recursive = RFECV(clf, min_features_to_select=1,cv=10,step=1)
        titleText = 'w. Cross Validation'
    recursive = recursive.fit(X, y)
    names = X.columns
    ranking = pd.Series(recursive.ranking_, index=names)
    ranking = ranking.sort_values(ascending=True)
    if rankLim == 0:
        rankLim = max(ranking)
    keep = ranking.index[ranking<=rankLim]
    eliminate = ranking.index[ranking>rankLim]
    if showFig == True:
        title = 'Recursive Feature Elimination '+titleText
        createFigure_k(keep,eliminate,ranking,title,'Ranking',vertical=False)
    if showElim == True:
        print('Features eliminated with RFE: %i'%eliminate)
        print(eliminate)
    X = X[keep]
    X['Damage'] = y
    return X

# ----------Dimensional Reduction Methods------------
def LDA_analysis(df, fig=False):
    X = df.drop(['Damage'], 1)
    y = df.Damage
    lda = LinearDiscriminantAnalysis(n_components=2)
    X_r2 = lda.fit(X, y).transform(X)
    # To include pca analysis to the test.
    pca = PCA(n_components=4)
    X_r = pca.fit(X).transform(X)
    # Denne funker ikke
    qda = QuadraticDiscriminantAnalysis()
    X_r3 = qda.fit(X, y)
    print(X_r3.score)
    if fig == True:
        figure, axs = plt.subplots(1, 3)
        axs[0].scatter(X_r[:, 0], X_r[:, 1], c=y)
        axs[1].scatter(X_r2[:, 0], X_r2[:, 1], c=y)
        axs[2].scatter(X_r[:, 0], X_r2[:, 0], c=y)
        plt.show()
        # # axs = plt.subplots(2, 1)
        # sns.boxplot(x='class', y='ld1', data=df)
        plt.show()

# Avventer litt med denne
def Genetic_analysis(df, method, fig=False):
    # print(np.shape(df.drop(['Damage'], 1)))
    E = np.random.uniform(0, 0.1, size=(700, 10))
    print(E)
    X = np.hstack((df.drop(['Damage'], 1), E))
    print(X)
    y = np.array(df.Damage)
    if method == 'LogReg':
        clf = LogisticRegression()
    elif method == 'DecisionTree':
        clf = DecisionTreeClassifier()
    elif method == 'RandomForest':
        clf = RandomForestClassifier(n_estimators=100, max_features=1)
    elif method == 'ExtremeTree':
        clf = ExtraTreesClassifier()
    elif method == 'XGB':
        clf = XGBClassifier()
    selector = GeneticSelectionCV(clf, cv=5, verbose=0, scoring="accuracy", max_features=5,
                                  n_population=40, crossover_proba=0.5, mutation_proba=0.2,
                                  n_generations=40, crossover_independent_proba=0.5,
                                  mutation_independent_proba=0.05, tournament_size=3,
                                  n_gen_no_change=10, caching=False, n_jobs=1)
    selector = selector.fit(X, y)
    print(selector.support_)


# ----------------------- DEFINE FEATURES AND DATAFRAMES USED ------------------------#

# splits = 50
    # Get features
# features = getFeatureList(splits)
# print(features)
# wantedFeatures = list(features[49:52]) + list(features[54:56])
# print(wantedFeatures)

    # Multiple Sensors
# df = combineSensors(getSensorList('l')[15:25], splits, wantedFeatures)
# print(df.shape[1])
    # Single Sensor
# df = pd.read_pickle('Features/Features_All/n_splits_50_sensor_AL02.pkl')
# df = chooseFeatures(df,wantedFeatures)

    # Preprocessing (If not removing constant)
# df = df_preprossecing(df)
# print(df.to_string())


# --------------------- SELECT METHODS FOR FEATURE SELECTION ------------------------#

# Create interactions between all features
# df = add_interactions(df)

    # Constant & Quasi-constant feature removal:
    # Checking and removing features with low variance
# df = check_variance_features(df, threshold=1e-13, showElim=True)
# print(df.columns)
# print(df.shape[1])
    # Preprocessing
# df = df_preprossecing(df)

    # Anova feature selection
# df = anova_FS(df, showFig=True, showElim=True)

    # Kruskal Wallis feature selection
# df = kruskal_wallis(df,showFig=True, showElim=True)

    # Univariate correlation
    # Methods: pearson, spearman, kendall
# df = uni_correlation(df, threshold=0.1, method='kendall', absVal=True, showFig=True, showElim=True)

    # Mutual Information
# df = mutual_information_gain_FS(df, threshold=0.4, showFig=True, showElim=True)

    # Multivariate correlation
    # Methods: pearson, spearman, kendall
# df = multi_correlation_V2(df, threshold=0.85, corr_method='spearman', showFig=False, showElim=True)
# df = multi_correlation_V2(df, threshold=0.85, corr_method='spearman', showFig=True, showElim=True)

    # ROC-AUC - See 'roc_auc_score' for description of methods.
    # multiClass: ovo, ovr
    # avgMethod: macro, weighted
    # ML: LogReg, DecisionTree, RandomForest, ExtremeTree, XGB
# df = ROC_AUC(df, threshold=0.1, testSize=0.5, ML='LogReg', avgMethod='macro', multiClass='ovo', auc_mod=True, showFig=True, showElim=True)

    # Model based Ranking
    # ML: LogReg, DecisionTree, RandomForest, ExtremeTree, XGB
# df = model_based_ranking(df, threshold=0.3, ML='DecicionTree', testSize=0.8, showFig=True, showElim=True)

# -----Wrapper Methods---------

    # Forward & Backward feature selection, with or without floating
    # ML: LogReg, DecisionTree, RandomForest, ExtremeTree, XGB
# df = wrapper_FS(df, forward=True, method='DecisionTree', floating=False, showFig=True, showElim=True)

# ------Embedded Methods-------

    # Regularization Methods
    # regu_methods = L1, L2, ElasticNet
    # Sensitiv for endringer i c
# df = regularisation_FS(df, regu_method='L1', showFig=True, showElim=True)

    # Feature importance using permutation of kNN
# df = kNN_importance(df, fig=True)

    # Tree based Feature Importance, MDI & MDA
    # ML: LogReg, DecisionTree, RandomForest, ExtremeTree, XGB
# df = tree_based_FS(df, k=0, ML='RandomForest', importance_method='MDA', showFig=True, showElim=True)

    # Boruta Feature elimination
    # ML: RandomForest, ExtremeTree, XGB
# df = boruta_FS(df, ML='ExtremeTree', showFig=True)

#--------Hybrid Methods---------

    # Recursive Feature Elimination'
    # Returns the ascending order of which features are best with 1 being best
    # CV True or False
    # ML: LogReg, DecisionTree, RandomForest, ExtremeTree, XGB
# df = recursive_feature_elimination(df, cv=True, rankLim = 3, method='RandomForest', showFig=True, showElim=True)

# ----------Dimensional Reduction Methods------------

# #LDA analysis
# LDA_test = LDA_analysis(df, fig=True)
